{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "from multiprocessing import Pool\n",
    "from progressbar import ProgressBar\n",
    "from unidecode import unidecode\n",
    "import string\n",
    "import pycountry\n",
    "import jellyfish\n",
    "import difflib\n",
    "import csv\n",
    "from collections import Counter\n",
    "import re\n",
    "import nltk\n",
    "from nltk import bigrams\n",
    "from nltk import trigrams\n",
    "import textual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle('entities_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'Nigeria': {'count': 2, 'probability': 1.0},\n",
       " u'United States': {'count': 12, 'probability': 1.0}}"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def correct_country_mispelling(s):\n",
    "    with open(\"data/ISO3166ErrorDictionary.csv\", \"rb\") as info:\n",
    "        reader = csv.reader(info)\n",
    "        for row in reader:\n",
    "            if s.lower() == unicode(row[0],'utf8').lower():\n",
    "                return unicode(row[2], 'utf8')\n",
    "            if unidecode(s).lower() == unidecode(unicode(row[0],'utf8')).lower():\n",
    "                return unicode(row[2], 'utf8')\n",
    "            if s.lower() == textual.remove_non_ascii(row[0]).lower():\n",
    "                return unicode(row[2], 'utf8')\n",
    "    return s\n",
    "\n",
    "\n",
    "def matching_countries(entity):\n",
    "    # further correction for misspellings\n",
    "    matched_countries = difflib.get_close_matches(entity, country_names, cutoff=0.8,)\n",
    "    if matched_countries:\n",
    "        confidence = difflib.SequenceMatcher(None, matched_countries[0], entity).ratio()\n",
    "        return (matched_countries[0], confidence)\n",
    "\n",
    "    \n",
    "def get_countries(places, spellcheck=False):\n",
    "    # correcting selling introduces some false positives\n",
    "    # likelihood of official government documents being spelled incorrectly is low\n",
    "    countries = []\n",
    "    for place, label in places:\n",
    "        if label in ['LOCATION', 'PERSON', 'ORGANIZATION']:\n",
    "            place = correct_country_mispelling(place)\n",
    "            if spellcheck:\n",
    "                match = matching_countries(place.lower())\n",
    "                if match:\n",
    "                    countries.append((place, match[1]))\n",
    "            else:\n",
    "                if place.lower() in country_names:\n",
    "                    countries.append((textual.titlecase(place), 1.0))\n",
    "    c = set(Counter(name for name, _ in countries).iteritems())\n",
    "    c_dict = {}\n",
    "    for country, count in c:\n",
    "        # gets the probability from before the counter\n",
    "        c_dict.update({country: {'probability': probability, 'count': count} for name, probability in sorted(countries) if name in country})\n",
    "    return c_dict\n",
    "\n",
    "get_countries(df.ix[8,'entities'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "country_names = [i.name for i in pycountry.countries]\n",
    "\n",
    "# fix country names \n",
    "def standardize_country_name(name):\n",
    "    try:\n",
    "        name = unicode(name, 'utf8')\n",
    "    except:\n",
    "        pass\n",
    "    name = correct_country_mispelling(name)\n",
    "    return name\n",
    "\n",
    "country_names = [standardize_country_name(i).lower() for i in country_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geoname_id</th>\n",
       "      <th>continent_code</th>\n",
       "      <th>continent_name</th>\n",
       "      <th>country_iso_code</th>\n",
       "      <th>country_name</th>\n",
       "      <th>subdivision_iso_code</th>\n",
       "      <th>subdivision_name</th>\n",
       "      <th>city_name</th>\n",
       "      <th>metro_code</th>\n",
       "      <th>time_zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1861060</td>\n",
       "      <td>AS</td>\n",
       "      <td>Asia</td>\n",
       "      <td>JP</td>\n",
       "      <td>Japan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Asia/Tokyo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1809858</td>\n",
       "      <td>AS</td>\n",
       "      <td>Asia</td>\n",
       "      <td>CN</td>\n",
       "      <td>China</td>\n",
       "      <td>44</td>\n",
       "      <td>Guangdong</td>\n",
       "      <td>Guangzhou</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Asia/Shanghai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1850147</td>\n",
       "      <td>AS</td>\n",
       "      <td>Asia</td>\n",
       "      <td>JP</td>\n",
       "      <td>Japan</td>\n",
       "      <td>13</td>\n",
       "      <td>Tōkyō</td>\n",
       "      <td>Tokyo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Asia/Tokyo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1814991</td>\n",
       "      <td>AS</td>\n",
       "      <td>Asia</td>\n",
       "      <td>CN</td>\n",
       "      <td>China</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2077456</td>\n",
       "      <td>OC</td>\n",
       "      <td>Oceania</td>\n",
       "      <td>AU</td>\n",
       "      <td>Australia</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   geoname_id continent_code continent_name country_iso_code country_name  \\\n",
       "0     1861060             AS           Asia               JP        Japan   \n",
       "1     1809858             AS           Asia               CN        China   \n",
       "2     1850147             AS           Asia               JP        Japan   \n",
       "3     1814991             AS           Asia               CN        China   \n",
       "4     2077456             OC        Oceania               AU    Australia   \n",
       "\n",
       "  subdivision_iso_code subdivision_name  city_name  metro_code      time_zone  \n",
       "0                  NaN              NaN        NaN         NaN     Asia/Tokyo  \n",
       "1                   44        Guangdong  Guangzhou         NaN  Asia/Shanghai  \n",
       "2                   13            Tōkyō      Tokyo         NaN     Asia/Tokyo  \n",
       "3                  NaN              NaN        NaN         NaN            NaN  \n",
       "4                  NaN              NaN        NaN         NaN            NaN  "
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subdivision_df = pd.DataFrame.from_csv('data/GeoLite2-City-Locations.csv', index_col=None, encoding='utf8').dropna(subset=['country_name'])\n",
    "subdivision_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s1 = subdivision_df[['country_name', 'subdivision_name']].dropna().rename(columns={'subdivision_name':'subdivision'})\n",
    "s1['type'] = 'subdivision'\n",
    "s2 = subdivision_df[['country_name', 'subdivision_iso_code']].dropna().rename(columns={'subdivision_iso_code':'subdivision'})\n",
    "s2['type'] = 'subdivision_code'\n",
    "s3 = subdivision_df[['country_name', 'city_name']].dropna().rename(columns={'city_name':'subdivision'})\n",
    "s3['type'] = 'city'\n",
    "s4 = subdivision_df[['country_name', 'country_iso_code']].dropna().rename(columns={'country_iso_code':'subdivision'})\n",
    "s4['type'] = 'country_code'\n",
    "\n",
    "# add countries to 'everything'\n",
    "s5 = pd.DataFrame([subdivision_df.country_name.unique()]*2).T\n",
    "s5.columns = ['country_name','subdivision']\n",
    "s5['type'] = 'country'\n",
    "\n",
    "almost_everything = pd.concat([s1,s2,s3,s4,s5], ignore_index=True).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country_name</th>\n",
       "      <th>subdivision</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>China</td>\n",
       "      <td>Guangdong</td>\n",
       "      <td>subdivision</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Japan</td>\n",
       "      <td>Tōkyō</td>\n",
       "      <td>subdivision</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Australia</td>\n",
       "      <td>Victoria</td>\n",
       "      <td>subdivision</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Thailand</td>\n",
       "      <td>Bangkok</td>\n",
       "      <td>subdivision</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Thailand</td>\n",
       "      <td>Changwat Samut Songkhram</td>\n",
       "      <td>subdivision</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  country_name               subdivision         type\n",
       "0        China                 Guangdong  subdivision\n",
       "1        Japan                     Tōkyō  subdivision\n",
       "2    Australia                  Victoria  subdivision\n",
       "3     Thailand                   Bangkok  subdivision\n",
       "4     Thailand  Changwat Samut Songkhram  subdivision"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "almost_everything.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import textual\n",
    "from nltk import word_tokenize, bigrams, trigrams\n",
    "import string\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tupled_everything = [tuple(x) for x in almost_everything[['country_name', 'subdivision']].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample = df.ix[14,'raw_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def check_for_subdivision(list_of_tuples, term):\n",
    "    matches = [(country, subdivision) for country, subdivision in list_of_tuples if subdivision == term]\n",
    "    return matches\n",
    "\n",
    "\n",
    "def adjust_probabilities(old_probability, possible_countries):\n",
    "    if len(set(count for _, count in possible_countries)) <= 1:\n",
    "        # no change to probabilities when there are no contextual clues\n",
    "        return [(country, old_probability) for country, _ in possible_countries]\n",
    "    \n",
    "    list_ = []\n",
    "    for country, count in possible_countries:\n",
    "        new_probability = old_probability\n",
    "        if count == 0:\n",
    "            # only decreases it by a single half if there is no nearby context for it\n",
    "            decrease = new_probability / 2\n",
    "            new_probability -= decrease\n",
    "        for i in range(count):\n",
    "            # increase probability by half for each context clue in range\n",
    "            increase = (1.0 - new_probability) / 2\n",
    "            new_probability += increase\n",
    "        list_.append((country, new_probability))\n",
    "    return list_\n",
    "\n",
    "\n",
    "def context_adjustment(place, possible_countries, probability, text):\n",
    "    contexts = get_contexts(place, text)\n",
    "    new_probabilities = []\n",
    "    \n",
    "    lump = []\n",
    "    for context in contexts:\n",
    "        # remove the original place from tokens\n",
    "        context = textual.remove_words(context, place)\n",
    "        tokens = nltk.word_tokenize(context)\n",
    "        # chop off first and last token which are likely not whole words\n",
    "        tokens = [token.lower() for token in tokens if token.isalpha()][1:-1]\n",
    "\n",
    "        bi_tokens = bigrams(tokens)\n",
    "        tri_tokens = trigrams(tokens)\n",
    "        tokens = tokens + [' '.join(t) for t in bi_tokens] + [' '.join(t) for t in tri_tokens]\n",
    "\n",
    "        # maintain capitalization of abbreviations and state codes\n",
    "#             tokens = [(lambda x: x if x == x.upper() else x.lower())(t) for t in tokens]\n",
    "        tokens = [(lambda x: x if x == x.upper() else textual.titlecase(x))(t) for t in tokens]\n",
    "        lump.extend(tokens)\n",
    "\n",
    "    # check whether contextual token is a country subdivision\n",
    "    context_countries = []\n",
    "    for token in lump:\n",
    "        context_countries.extend([country for country, subdivision in tupled_everything if subdivision == token])\n",
    "\n",
    "    # use the existance of contextual countries to clarify ambiguous countries\n",
    "    # if you dont take the set then you end up with false positives from multiple copies of same wrong country\n",
    "    if context_countries:\n",
    "        context_count = Counter(set(context_countries))\n",
    "#                 print('Counts for each context-country are {}'.format(context_count))\n",
    "        ambiguous_country_counts = zip(possible_countries, map(lambda x: context_count[x], possible_countries))\n",
    "#                 print('Counts for ambiguous countries are {}'.format(ambiguous_country_counts))\n",
    "        new_probabilities.extend(adjust_probabilities(probability, ambiguous_country_counts))\n",
    "\n",
    "    # combine multiple contexts into a single count and probability per country \n",
    "    dict_ = {}\n",
    "    if new_probabilities:\n",
    "        country_set = {i[0] for i in new_probabilities}\n",
    "        for country in country_set:\n",
    "            probs = [i[1] for i in new_probabilities if i[0] == country]\n",
    "            count = len(probs)\n",
    "            probability = probs.pop(0)\n",
    "            if probs:\n",
    "                for i in probs:\n",
    "                    probability = independent_either_probability(probability, i)\n",
    "            dict_[country] = {'count': count, 'probability': probability}\n",
    "    else:\n",
    "        for country in possible_countries:\n",
    "            dict_[country] = {'count': 1, 'probability': probability}\n",
    "    return dict_\n",
    "\n",
    "\n",
    "def independent_either_probability(oldp, newp):\n",
    "    probability_non_occurrence = (1-oldp) * (1-newp)\n",
    "    new_probability = 1 - probability_non_occurrence\n",
    "    return new_probability\n",
    "\n",
    "\n",
    "def update_countries_with_regions(entities, found_countries, text):\n",
    "    # adds countries derived from regions to country list\n",
    "    ambiguous_locations = {}\n",
    "    \n",
    "    matches = []\n",
    "    \n",
    "    for entity in {i[0] for i in entities if i[1]=='LOCATION'}:\n",
    "        matches.extend(check_for_subdivision(tupled_everything, entity))\n",
    "\n",
    "    if matches:\n",
    "        matches = [(standardize_country_name(country), subdivision) for country, subdivision in matches]\n",
    "        no_dupes = set(matches)\n",
    "        tokenized = False\n",
    "        for place, count in Counter(i[1] for i in no_dupes).items():\n",
    "            probability = 1.0 / count\n",
    "            possible_countries = [country for country, subdivision in matches if subdivision == place]\n",
    "            if count == 1:\n",
    "                # only one country exists for a single subdivision\n",
    "                probability = 0.8 # correcting for imperfect entity parsing\n",
    "                country = possible_countries[0]\n",
    "                if country in found_countries:\n",
    "                    priors = found_countries[country]\n",
    "                    new_count = priors['count'] + len(possible_countries)\n",
    "                    new_probability = independent_either_probability(priors['probability'], probability)\n",
    "                    found_countries[country] = {'count': new_count, 'probability': new_probability}\n",
    "                else:\n",
    "                    found_countries[country] = {'count': len(possible_countries), 'probability': probability}\n",
    "            else:\n",
    "                # multiple countries exist for a single subdivision                  \n",
    "                possible_countries = set(possible_countries)\n",
    "                new_probabilities = context_adjustment(place, possible_countries, probability, text)\n",
    "                ambiguous_locations[place] = {'possible_countries': possible_countries}\n",
    "                for country in possible_countries:\n",
    "                    if country in found_countries:\n",
    "                        priors = found_countries[country]\n",
    "                        new_count = priors['count'] + new_probabilities[country]['count']\n",
    "                        new_probability = independent_either_probability(priors['probability'], new_probabilities[country]['probability'])\n",
    "                        found_countries[country] = {'count': new_count, 'probability': new_probability}\n",
    "                    else:\n",
    "                        found_countries[country] = {'count': new_probabilities[country]['count'], 'probability': new_probabilities[country]['probability']}\n",
    "    return found_countries, ambiguous_locations\n",
    "\n",
    "\n",
    "def get_contexts(term, text):\n",
    "    # get contextual windows revolving around ambiguous terms\n",
    "    window = 60\n",
    "    bottom = lambda x: x-window if x-window > 0 else 0\n",
    "    top = lambda x: x+window if x+window < len(text) else len(text)\n",
    "    indices = list(textual.find_all(text, term))\n",
    "    contexts = [text[bottom(i):top(i)] for i in indices]\n",
    "    return contexts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({u'Australia': {'count': 1, 'probability': 0.5},\n",
       "  u'Nigeria': {'count': 3, 'probability': 1.0},\n",
       "  u'Portugal': {'count': 1, 'probability': 0.25},\n",
       "  u'United States': {'count': 16, 'probability': 1.0}},\n",
       " {u'Lisbon': {'possible_countries': {u'Portugal', u'United States'}},\n",
       "  u'Maryland': {'possible_countries': {u'Australia', u'United States'}}})"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_countries(df.ix[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parse_countries(row):\n",
    "    countries = get_countries(row.entities)\n",
    "    text = row.title + '\\n' + row.toc_subject + '\\n' + ' '.join(row.topics) + '\\n' + row.raw_text\n",
    "    countries, ambiguous_locations = update_countries_with_regions(row.entities, countries, text)\n",
    "    return (countries, ambiguous_locations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***\n",
      "0\n",
      "[0.1377551020408163, 0.4375, 1.0, 0.1377551020408163, 0.1377551020408163, 0.1377551020408163, 0.1377551020408163, 0.1377551020408163, 0.1377551020408163, 0.1377551020408163, 1.0, 0.4375, 0.1377551020408163, 0.1377551020408163, 0.1377551020408163, 0.1377551020408163]\n",
      "[0.1377551020408163, 0.4375, 1.0, 0.1377551020408163, 0.1377551020408163, 0.1377551020408163, 0.1377551020408163, 0.1377551020408163, 0.1377551020408163, 0.1377551020408163, 1.0, 0.4375, 0.1377551020408163, 0.1377551020408163, 0.1377551020408163, 0.1377551020408163]\n",
      "***\n",
      "1\n",
      "[1.0]\n",
      "[1.0]\n",
      "***\n",
      "2\n",
      "[0.15972222222222232, 0.826388888888889, 0.15972222222222232, 0.9722222222222222, 0.18999999999999995, 0.15972222222222232, 1.0, 0.18999999999999995, 0.18999999999999995]\n",
      "[0.15972222222222232, 0.826388888888889, 0.15972222222222232, 0.9722222222222222, 0.18999999999999995, 0.15972222222222232, 1.0, 0.18999999999999995, 0.18999999999999995]\n",
      "***\n",
      "3\n",
      "[0.4375, 1.0, 1.0, 1.0, 0.4375, 1.0]\n",
      "[0.4375, 1.0, 1.0, 1.0, 0.4375, 1.0]\n",
      "***\n",
      "4\n",
      "[0.999999998976, 0.8400000000000001, 0.18999999999999995, 0.18999999999999995, 0.8400000000000001]\n",
      "[0.999999998976, 0.8400000000000001, 0.18999999999999995, 0.18999999999999995, 0.8400000000000001]\n",
      "***\n",
      "5\n",
      "[0.99999755859375, 0.4375, 0.4375]\n",
      "[0.99999755859375, 0.4375, 0.4375]\n",
      "***\n",
      "6\n",
      "[0.999999894036187, 0.4375, 1.0, 0.4375, 0.30555555555555547]\n",
      "[0.999999894036187, 0.4375, 1.0, 0.4375, 0.30555555555555547]\n",
      "***\n",
      "7\n",
      "[1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "[1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "***\n",
      "8\n",
      "[1.0, 1.0, 0.75, 0.4375]\n",
      "[1.0, 1.0, 0.75, 0.4375]\n",
      "***\n",
      "9\n",
      "[0.888888888888889, 0.30555555555555547, 0.30555555555555547, 0.30555555555555547, 0.3599999999999999, 0.30555555555555547, 0.3599999999999999, 0.3599999999999999, 1.0, 0.30555555555555547, 0.96, 0.3599999999999999]\n",
      "[0.888888888888889, 0.30555555555555547, 0.30555555555555547, 0.30555555555555547, 0.3599999999999999, 0.30555555555555547, 0.3599999999999999, 0.3599999999999999, 1.0, 0.30555555555555547, 0.96, 0.3599999999999999]\n",
      "***\n",
      "10\n",
      "[1.0, 1.0]\n",
      "[1.0, 1.0]\n",
      "***\n",
      "11\n",
      "[1.0, 1.0, 1.0]\n",
      "[1.0, 1.0, 1.0]\n",
      "***\n",
      "12\n",
      "[1.0, 1.0, 0.75]\n",
      "[1.0, 1.0, 0.75]\n",
      "***\n",
      "13\n",
      "[1.0, 1.0, 0.609375, 0.4375]\n",
      "[1.0, 1.0, 0.609375, 0.4375]\n",
      "***\n",
      "14\n",
      "[1.0, 0.4375, 1.0, 0.4375, 0.30555555555555547]\n",
      "[1.0, 0.4375, 1.0, 0.4375, 0.30555555555555547]\n",
      "***\n",
      "15\n",
      "[1.0, 0.4375, 0.4375]\n",
      "[1.0, 0.4375, 0.4375]\n",
      "***\n",
      "16\n",
      "[0.9975, 0.4375]\n",
      "[0.9975, 0.4375]\n",
      "***\n",
      "17\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4375, 1.0, 0.4375, 1.0, 1.0]\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4375, 1.0, 0.4375, 1.0, 1.0]\n",
      "***\n",
      "18\n",
      "[1.0, 0.234375, 0.234375, 0.234375]\n",
      "[1.0, 0.234375, 0.234375, 0.234375]\n",
      "***\n",
      "19\n",
      "[1.0, 0.68359375, 0.30555555555555547, 0.609375, 0.8163265306122449, 0.1377551020408163, 0.96, 0.1377551020408163, 0.3599999999999999, 0.3599999999999999, 0.9375, 1.0, 0.30555555555555547, 0.4375, 0.30555555555555547, 0.96, 0.3599999999999999, 0.8824489795918367, 0.1377551020408163]\n",
      "[1.0, 0.68359375, 0.30555555555555547, 0.609375, 0.8163265306122449, 0.1377551020408163, 0.96, 0.1377551020408163, 0.3599999999999999, 0.3599999999999999, 0.9375, 1.0, 0.30555555555555547, 0.4375, 0.30555555555555547, 0.96, 0.3599999999999999, 0.8824489795918367, 0.1377551020408163]\n",
      "***\n",
      "20\n",
      "[1.0, 1.0, 0.4375, 0.4375]\n",
      "[1.0, 1.0, 0.4375, 0.4375]\n"
     ]
    }
   ],
   "source": [
    "sample = df.ix[0:20]\n",
    "for row in sample.iterrows():\n",
    "    print '***'\n",
    "    print row[0]\n",
    "    print parse_countries(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pbar = ProgressBar(maxval=df.shape[0]).start()\n",
    "countries = []\n",
    "for ix, row in enumerate(df.iterrows()):\n",
    "    countries.append(parse_countries(row[1]))\n",
    "    pbar.update(ix)\n",
    "pbar.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_text_url</th>\n",
       "      <th>title</th>\n",
       "      <th>toc_subject</th>\n",
       "      <th>topics</th>\n",
       "      <th>raw_text</th>\n",
       "      <th>entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>https://www.federalregister.gov/articles/text/...</td>\n",
       "      <td>Fresh Garlic From the People's Republic of Chi...</td>\n",
       "      <td>Antidumping Duty New Shipper Reviews; Results,...</td>\n",
       "      <td>[]</td>\n",
       "      <td>\\nSUMMARY: \\nThe Department of Commerce (Depar...</td>\n",
       "      <td>[(Department of Commerce ( Department, ORGANIZ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         raw_text_url  \\\n",
       "14  https://www.federalregister.gov/articles/text/...   \n",
       "\n",
       "                                                title  \\\n",
       "14  Fresh Garlic From the People's Republic of Chi...   \n",
       "\n",
       "                                          toc_subject topics  \\\n",
       "14  Antidumping Duty New Shipper Reviews; Results,...     []   \n",
       "\n",
       "                                             raw_text  \\\n",
       "14  \\nSUMMARY: \\nThe Department of Commerce (Depar...   \n",
       "\n",
       "                                             entities  \n",
       "14  [(Department of Commerce ( Department, ORGANIZ...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.raw_text_url == 'https://www.federalregister.gov/articles/text/raw_text/201/231/447.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u\"\\nSUMMARY: \\nThe Department of Commerce (Department) has determined that a request for a new shipper review (NSR) under the antidumping duty order on fresh garlic from the People's Republic of China (PRC) meets the statutory and regulatory requirements for initiation. The period of review (POR) is November 1, 2011, through October 31, 2012. \\nDATES: \\nEffective Date: January 2, 2012. \\nFOR FURTHER INFORMATION CONTACT: \\nLingjun Wang, AD/CVD Operations, Office 6, Import Administration, International Trade Administration, U.S. Department of Commerce, 14th Street and Constitution Avenue NW., Washington, DC 20230; telephone: (202) 482-2316. \\nSUPPLEMENTARY INFORMATION: \\nBackground \\nThe Department published the antidumping duty order on fresh garlic from the PRC in the Federal Register on November 16, 1994. 1 \\nOn November 27, 2012, the Department received a timely NSR request from Shijiazhuang Goodman Trading Co., Ltd. (Goodman) in accordance with section 751(a)(2)(B)(i) of the Tariff Act of 1930, as amended (the Act), and 19 CFR 351.214(c). \\nPursuant to the requirements set forth in 19 CFR 351.214(b), Goodman certified that it is the exporter and Jinxiang Zhongtian Business Co., Ltd. (a.k.a. Jinxiang Zhongtian Trade Co., Ltd.) (Zhongtian) certified that it is the producer of the fresh garlic exported by Goodman. Moreover, Goodman and Zhongtian each certified that: (1) They did not export fresh garlic for sale to the United States during the period of investigation (POI); (2) since the investigation was initiated, they have never been affiliated with any exporter or producer who exported the subject merchandise to the United States during the POI, including those not individually examined during the investigation; and (3) their export activities are not controlled by the central government of the PRC. In addition, Goodman submitted documentation establishing the following: (1) The date on which fresh garlic was first entered; (2) the volume of that and subsequent shipments; and (3) the date of the first sale to an unaffiliated customer in the United States. \\nThe Department queried the database of U.S. Customs and Border Protection (CBP) in an attempt to confirm that shipments reported by Goodman had entered the United States for consumption and that liquidation had been properly suspended for antidumping duties. The information which the Department examined was consistent with that provided by Goodman in its request. 2 \\nPeriod of Review \\nIn accordance with 19 CFR 351.214(g)(1)(i)(A), the POR for an NSR initiated in the month immediately following the anniversary month will be the twelve-month period immediately preceding the anniversary month. Therefore, the POR for this NSR is November 1, 2011, through October 31, 2012. The sales and entries into the United States of subject merchandise exported by Goodman and produced by Zhongtian occurred during this twelve-month POR. \\nInitiation of New Shipper Review \\nPursuant to section 751(a)(2)(B) of the Act and 19 CFR 351.214(b), and the information on the record, the Department finds that Goodman's request meets the threshold requirements for initiation of an NSR. The Department intends to issue the preliminary result within 180 days after the date on which the review is initiated, and the final results within 90 days after the date on which we issue the preliminary results. 3 \\nIt is the Department's usual practice, in cases involving non-market economies, to require that a company seeking to establish eligibility for an antidumping duty rate separate from the country-wide rate ( i.e., a separate rate) provide evidence of de jure and de facto absence of government control over the company's export activities. Accordingly, the Department will issue a questionnaire to Goodman that includes a separate rate section. The review will proceed if the response provides sufficient indication that the exporter and producer are not subject to either de jure or de facto government control with respect to their export of fresh garlic. \\nThe Department will instruct CBP to allow, at the option of the importer, the posting, until the completion of the review, of a bond or security in lieu of a cash deposit for certain entries of the subject merchandise from Goodman in accordance with section 751(a)(2)(B)(iii) of the Act and 19 CFR 351.214(e). Specifically, the bonding privilege will only apply to entries of subject merchandise exported by Goodman and produced by Zhongtian, the sales of which are the basis for this NSR request. \\nInterested parties requiring access to proprietary information in this NSR should submit applications for disclosure under administrative protective order in accordance with 19 CFR 351.305 and 351.306. \\nThis initiation and notice are in accordance with section 751(a)(2)(B) of the Act and 19 CFR 351.214 and 351.221(c)(1)(i). \\nDated: December 21, 2012. \\nChristian Marsh, \\nDeputy Assistant Secretary for Antidumping and Countervailing Duty Operations. \\n[FR Doc. 2012-31447 Filed 12-31-12; 8:45 am] \\nBILLING CODE 3510-DS-P \\n\""
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.ix[14, 'raw_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({u'China': {'count': 2, 'probability': 1.0},\n",
       "  u'Colombia': {'count': 1, 'probability': 0.25},\n",
       "  u'Russian Federation': {'count': 1, 'probability': 0.16666666666666666},\n",
       "  u'United Kingdom': {'count': 1, 'probability': 0.25},\n",
       "  u'United States': {'count': 9, 'probability': 1.0}},\n",
       " {u'China': {'possible_countries': [u'United States',\n",
       "    u'Russian Federation',\n",
       "    u'China']},\n",
       "  u'DC': {'possible_countries': [u'United States', u'Colombia']},\n",
       "  u'Washington': {'possible_countries': [u'United States',\n",
       "    u'United Kingdom']}})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countries = get_countries(df.ix[14,'entities'])\n",
    "update_countries_with_regions(df.ix[14,'entities'], countries, df.ix[14,'raw_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "{u'China': {'count': 1, 'probability': 0.16666666666666666},\n",
    " u'Colombia': {'count': 1, 'probability': 0.25},\n",
    " u'Russian Federation': {'count': 1, 'probability': 0.16666666666666666},\n",
    " u'United Kingdom': {'count': 1, 'probability': 0.25},\n",
    " u'United States': {'count': 4, 'probability': 0.9999674479166667}}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
