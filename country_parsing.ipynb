{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "from multiprocessing import Pool\n",
    "from progressbar import ProgressBar\n",
    "from unidecode import unidecode\n",
    "import string\n",
    "import pycountry\n",
    "import jellyfish\n",
    "import difflib\n",
    "import csv\n",
    "from collections import Counter\n",
    "import re\n",
    "import nltk\n",
    "from nltk import bigrams\n",
    "from nltk import trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle('entities_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_non_ascii(s): return \"\".join(i for i in s if ord(i)<128)\n",
    " \n",
    "def fuzzy_match(s1, s2, max_dist=.8):\n",
    "    return jellyfish.jaro_distance(s1, s2) >= max_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'China': {'count': 1, 'probability': 1.0},\n",
       " u'United States': {'count': 5, 'probability': 1.0}}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def title_except(s, exceptions=['a', 'an', 'the', 'and', 'but', 'or', 'for' 'nor', 'on', 'at', 'to', 'from', 'by', 'of']):\n",
    "    word_list = re.split(' ', s)\n",
    "    final = []\n",
    "    for ix, word in enumerate(word_list):\n",
    "        if word.count('.') > 1:\n",
    "            # fix abbreviations correctly\n",
    "            word = word.upper()\n",
    "        elif '-' in word:\n",
    "            location = word.find('-')\n",
    "            word = word.capitalize()\n",
    "            word = word.replace(word[location+1], word[location+1].upper())\n",
    "        elif word[0] in ['(', '[']:\n",
    "            word = word.replace(word[1], word[1].upper())\n",
    "        elif ix == 0:\n",
    "            word = word.capitalize()\n",
    "        elif word in exceptions:\n",
    "            word = word.lower()\n",
    "        elif \"d'ivoire\" in word.lower():\n",
    "            word = \"d'Ivoire\"\n",
    "        else:\n",
    "            word = word.capitalize()\n",
    "        final.append(word)\n",
    "    return \" \".join(final)\n",
    "\n",
    "\n",
    "def correct_country_mispelling(s):\n",
    "    with open(\"ISO3166ErrorDictionary.csv\", \"rb\") as info:\n",
    "        reader = csv.reader(info)\n",
    "        for row in reader:\n",
    "            if s.lower() == unicode(row[0],'utf8').lower():\n",
    "                return unicode(row[2], 'utf8')\n",
    "            if unidecode(s).lower() == unidecode(unicode(row[0],'utf8')).lower():\n",
    "                return unicode(row[2], 'utf8')\n",
    "            if s.lower() == remove_non_ascii(row[0]).lower():\n",
    "                return unicode(row[2], 'utf8')\n",
    "    return s\n",
    "\n",
    "\n",
    "def matching_countries(entity):\n",
    "    # further correction for misspellings\n",
    "    matched_countries = difflib.get_close_matches(entity, country_names, cutoff=0.8,)\n",
    "    if matched_countries:\n",
    "        confidence = difflib.SequenceMatcher(None, matched_countries[0], entity).ratio()\n",
    "        return (matched_countries[0], confidence)\n",
    "\n",
    "    \n",
    "def get_countries(places, spellcheck=False):\n",
    "    # correcting selling introduces some false positives\n",
    "    # likelihood of official government documents being spelled incorrectly is low\n",
    "    countries = []\n",
    "    for place, label in places:\n",
    "        if label in ['LOCATION', 'PERSON', 'ORGANIZATION']:\n",
    "            place = correct_country_mispelling(place)\n",
    "            if spellcheck:\n",
    "                match = matching_countries(place.lower())\n",
    "                if match:\n",
    "                    countries.append((place, match[1]))\n",
    "            else:\n",
    "                if place.lower() in country_names:\n",
    "                    countries.append((title_except(place), 1.0))\n",
    "    c = set(Counter(name for name, _ in countries).iteritems())\n",
    "    c_dict = {}\n",
    "    for country, count in c:\n",
    "        # gets the probability from before the counter\n",
    "        c_dict.update({country: {'probability': probability, 'count': count} for name, probability in sorted(countries) if name in country})\n",
    "    return c_dict\n",
    "\n",
    "get_countries(df.ix[14,'entities'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "country_names = [i.name for i in pycountry.countries]\n",
    "\n",
    "# fix country names \n",
    "def standardize_country_name(name):\n",
    "    try:\n",
    "        name = unicode(name, 'utf8')\n",
    "    except:\n",
    "        pass\n",
    "    name = correct_country_mispelling(name)\n",
    "    return name\n",
    "\n",
    "country_names = [standardize_country_name(i).lower() for i in country_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geoname_id</th>\n",
       "      <th>continent_code</th>\n",
       "      <th>continent_name</th>\n",
       "      <th>country_iso_code</th>\n",
       "      <th>country_name</th>\n",
       "      <th>subdivision_iso_code</th>\n",
       "      <th>subdivision_name</th>\n",
       "      <th>city_name</th>\n",
       "      <th>metro_code</th>\n",
       "      <th>time_zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1861060</td>\n",
       "      <td>AS</td>\n",
       "      <td>Asia</td>\n",
       "      <td>JP</td>\n",
       "      <td>Japan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Asia/Tokyo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1809858</td>\n",
       "      <td>AS</td>\n",
       "      <td>Asia</td>\n",
       "      <td>CN</td>\n",
       "      <td>China</td>\n",
       "      <td>44</td>\n",
       "      <td>Guangdong</td>\n",
       "      <td>Guangzhou</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Asia/Shanghai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1850147</td>\n",
       "      <td>AS</td>\n",
       "      <td>Asia</td>\n",
       "      <td>JP</td>\n",
       "      <td>Japan</td>\n",
       "      <td>13</td>\n",
       "      <td>Tōkyō</td>\n",
       "      <td>Tokyo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Asia/Tokyo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1814991</td>\n",
       "      <td>AS</td>\n",
       "      <td>Asia</td>\n",
       "      <td>CN</td>\n",
       "      <td>China</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2077456</td>\n",
       "      <td>OC</td>\n",
       "      <td>Oceania</td>\n",
       "      <td>AU</td>\n",
       "      <td>Australia</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   geoname_id continent_code continent_name country_iso_code country_name  \\\n",
       "0     1861060             AS           Asia               JP        Japan   \n",
       "1     1809858             AS           Asia               CN        China   \n",
       "2     1850147             AS           Asia               JP        Japan   \n",
       "3     1814991             AS           Asia               CN        China   \n",
       "4     2077456             OC        Oceania               AU    Australia   \n",
       "\n",
       "  subdivision_iso_code subdivision_name  city_name  metro_code      time_zone  \n",
       "0                  NaN              NaN        NaN         NaN     Asia/Tokyo  \n",
       "1                   44        Guangdong  Guangzhou         NaN  Asia/Shanghai  \n",
       "2                   13            Tōkyō      Tokyo         NaN     Asia/Tokyo  \n",
       "3                  NaN              NaN        NaN         NaN            NaN  \n",
       "4                  NaN              NaN        NaN         NaN            NaN  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subdivision_df = pd.DataFrame.from_csv('GeoLite2-City-Locations.csv', index_col=None, encoding='utf8').dropna(subset=['country_name'])\n",
    "subdivision_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s1 = subdivision_df[['country_name', 'subdivision_name']].dropna().rename(columns={'subdivision_name':'subdivision'})\n",
    "s1['type'] = 'subdivision'\n",
    "s2 = subdivision_df[['country_name', 'subdivision_iso_code']].dropna().rename(columns={'subdivision_iso_code':'subdivision'})\n",
    "s2['type'] = 'subdivision_code'\n",
    "s3 = subdivision_df[['country_name', 'city_name']].dropna().rename(columns={'city_name':'subdivision'})\n",
    "s3['type'] = 'city'\n",
    "s4 = subdivision_df[['country_name', 'country_iso_code']].dropna().rename(columns={'country_iso_code':'subdivision'})\n",
    "s4['type'] = 'country_code'\n",
    "\n",
    "# add countries to 'everything'\n",
    "s5 = pd.DataFrame([subdivision_df.country_name.unique()]*2).T\n",
    "s5.columns = ['country_name','subdivision']\n",
    "s5['type'] = 'country'\n",
    "\n",
    "almost_everything = pd.concat([s1,s2,s3,s4,s5], ignore_index=True).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country_name</th>\n",
       "      <th>subdivision</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>China</td>\n",
       "      <td>Guangdong</td>\n",
       "      <td>subdivision</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Japan</td>\n",
       "      <td>Tōkyō</td>\n",
       "      <td>subdivision</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Australia</td>\n",
       "      <td>Victoria</td>\n",
       "      <td>subdivision</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Thailand</td>\n",
       "      <td>Bangkok</td>\n",
       "      <td>subdivision</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Thailand</td>\n",
       "      <td>Changwat Samut Songkhram</td>\n",
       "      <td>subdivision</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  country_name               subdivision         type\n",
       "0        China                 Guangdong  subdivision\n",
       "1        Japan                     Tōkyō  subdivision\n",
       "2    Australia                  Victoria  subdivision\n",
       "3     Thailand                   Bangkok  subdivision\n",
       "4     Thailand  Changwat Samut Songkhram  subdivision"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "almost_everything.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def adjust_probabilities(old_probability, possible_countries):\n",
    "    if len(set(count for _, count in possible_countries)) <= 1:\n",
    "        # no change to probabilities when there are no contextual clues\n",
    "        return [(country, old_probability) for country, _ in possible_countries]\n",
    "    \n",
    "    list_ = []\n",
    "    for country, count in possible_countries:\n",
    "        new_probability = old_probability\n",
    "        if count == 0:\n",
    "            # only decreases it by a single half if there is no nearby context for it\n",
    "            decrease = new_probability / 2\n",
    "            new_probability -= decrease\n",
    "        for i in range(count):\n",
    "            # increase probability by half for each context clue in range\n",
    "            increase = (1.0 - new_probability) / 2\n",
    "            new_probability += increase\n",
    "        list_.append((country, new_probability))\n",
    "    return list_\n",
    "\n",
    "\n",
    "def remove_word(s, word):\n",
    "    remove = word\n",
    "    regex = re.compile(r'\\b('+remove+r')\\b', flags=re.IGNORECASE)\n",
    "    out = regex.sub(\"\", s)\n",
    "    return out\n",
    "\n",
    "\n",
    "def find_all(a_str, sub):\n",
    "    start = 0\n",
    "    while True:\n",
    "        start = a_str.find(sub, start)\n",
    "        if start == -1: return\n",
    "        yield start\n",
    "        start += len(sub) # use start += 1 to find overlapping matches\n",
    "\n",
    "\n",
    "def context_adjustment(place, possible_countries, probability, text):\n",
    "    # get contextual windows revolving around ambiguous place name\n",
    "#     print('{} could be in {} with a probability of {} for each'.format(place, possible_countries, probability))\n",
    "    window = 60\n",
    "    bottom = lambda x: x-window if x-window > 0 else 0\n",
    "    top = lambda x: x+window if x+window < len(text) else len(text)\n",
    "#     print indices\n",
    "    indices = list(find_all(text, place))\n",
    "    contexts = [text[bottom(i):top(i)] for i in indices]    \n",
    "#     print('{} has surrounding contexts of {}'.format(place, contexts))\n",
    "#     print\n",
    "    new_probabilities = []\n",
    "    while not new_probabilities:\n",
    "        # waits until any contextual clues are acquired rather than getting every possible contextual clue which can lead to false positives when get multiple copies of same error\n",
    "        for context in contexts:\n",
    "            context = remove_word(context, place)\n",
    "            tokens = nltk.word_tokenize(context)\n",
    "            codes = [t for t in tokens if t==t.upper() and t.isalpha()]\n",
    "\n",
    "            # chop off first and last token which are likely not whole words\n",
    "            tokens = [token.lower() for token in tokens if token.isalpha()][1:-1]\n",
    "            bi_tokens = bigrams(tokens)\n",
    "            tri_tokens = trigrams(tokens)\n",
    "            tokens = tokens + [' '.join(t) for t in bi_tokens] + [' '.join(t) for t in tri_tokens]\n",
    "\n",
    "            # fix capitalization of state codes\n",
    "            tokens = [(lambda x: x.upper() if x.upper() in codes else title_except(x))(t) for t in tokens]\n",
    "#             print('Recognized locations in the context are {}'.format(filter(lambda x: x in [i for i in almost_everything.subdivision.tolist()], tokens)))\n",
    "            context_countries = []\n",
    "\n",
    "            # check whether contextual token is a country subdivision\n",
    "            for i in tokens:\n",
    "                a = almost_everything[almost_everything.subdivision == i]\n",
    "                if not a.empty:\n",
    "                    list_ = a.country_name.tolist()\n",
    "                    context_countries.extend(list_)\n",
    "    #                 print('{} could refer to {}'.format(i, list_))\n",
    "\n",
    "            # use the number of contextual countries that are the same as the ambiguous countries to compute new probabilities\n",
    "            if context_countries:\n",
    "                context_count = Counter(context_countries)\n",
    "    #             print('Counts for each context-country are {}'.format(context_count))\n",
    "                ambiguous_country_counts = zip(possible_countries, map(lambda x: context_count[x], possible_countries))\n",
    "#                 print('Counts for ambiguous countries are {}'.format(ambiguous_country_counts))\n",
    "                new_probabilities.extend(adjust_probabilities(probability, ambiguous_country_counts))\n",
    "                break # break out of for loop when gather first contextual clue\n",
    "        break # break out of while loop when there are no contextual clues after looping through all\n",
    "\n",
    "    # combine multiple contexts into a single count and probability per country\n",
    "    dict_ = {}\n",
    "    if new_probabilities:\n",
    "        country_set = {i[0] for i in new_probabilities}\n",
    "        for country in country_set:\n",
    "            probs = [i[1] for i in new_probabilities if i[0] == country]\n",
    "            count = len(probs)\n",
    "            probability = probs.pop(0)\n",
    "            if probs:\n",
    "                for i in probs:\n",
    "                    probability = independent_either_probability(probability, i)\n",
    "            dict_[country] = {'count': count, 'probability': probability}\n",
    "    else:\n",
    "        for country in possible_countries:\n",
    "            dict_[country] = {'count': 1, 'probability': probability}\n",
    "    return dict_\n",
    "\n",
    "\n",
    "def independent_either_probability(oldp, newp):\n",
    "    probability_non_occurrence = (1-oldp) * (1-newp)\n",
    "    new_probability = 1 - probability_non_occurrence\n",
    "    return new_probability\n",
    "\n",
    "\n",
    "def update_countries_with_regions(entities, countries, text):\n",
    "    # adds countries derived from regions to country list\n",
    "    ambiguous_locations = {}\n",
    "    \n",
    "    subs = pd.DataFrame()\n",
    "    for entity in {i[0] for i in entities if i[1]=='LOCATION'}:\n",
    "        a = almost_everything[almost_everything.subdivision == entity]\n",
    "        if not a.empty:\n",
    "            subs = pd.concat([subs, a], ignore_index=True)\n",
    "    \n",
    "    if not subs.empty:\n",
    "        subs.country_name = subs.country_name.apply(standardize_country_name)\n",
    "        no_dupes = subs.drop_duplicates(['country_name', 'subdivision'])\n",
    "        for value_count in no_dupes.subdivision.value_counts().iteritems():\n",
    "            count = value_count[1]\n",
    "            place = value_count[0]\n",
    "            probability = 1.0 / count\n",
    "            if probability == 1.0:\n",
    "                # only one country exists for a single subdivision\n",
    "                probability = 0.8 # correcting for imperfect entity parsing\n",
    "                possible_countries = subs[subs.subdivision == place].country_name.tolist()\n",
    "                country = possible_countries[0]\n",
    "                if country in countries:\n",
    "                    priors = countries[country]\n",
    "                    new_count = priors['count'] + len(possible_countries)\n",
    "                    new_probability = independent_either_probability(priors['probability'], probability)\n",
    "                    countries[country] = {'count': new_count, 'probability': new_probability}\n",
    "                else:\n",
    "                    countries[country] = {'count': len(possible_countries), 'probability': probability}\n",
    "            else:\n",
    "                # multiple countries exist for a single subdivision\n",
    "                possible_countries = no_dupes[no_dupes.subdivision == place].country_name.tolist()\n",
    "                new_probabilities = context_adjustment(place, possible_countries, probability, text)\n",
    "                ambiguous_locations[place] = {'possible_countries': possible_countries}\n",
    "                for country in possible_countries:\n",
    "                    if country in countries:\n",
    "                        priors = countries[country]\n",
    "                        new_count = priors['count'] + new_probabilities[country]['count']\n",
    "                        new_probability = independent_either_probability(priors['probability'], new_probabilities[country]['probability'])\n",
    "                        countries[country] = {'count': new_count, 'probability': new_probability}\n",
    "                    else:\n",
    "                        countries[country] = {'count': new_probabilities[country]['count'], 'probability': new_probabilities[country]['probability']}\n",
    "    return countries, ambiguous_locations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parse_countries(row):\n",
    "    countries = get_countries(row.entities)\n",
    "    text = row.title + '\\n' + row.toc_subject + '\\n' + ' '.join(row[1].topics) + '\\n' + row.raw_text\n",
    "    countries, ambiguous_locations = update_countries_with_regions(row.entities, countries, text)\n",
    "    return (countries, ambiguous_locations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***\n",
      "0\n",
      "{u'Canada': {'count': 1, 'probability': 0.07142857142857142}, u'United Kingdom': {'count': 1, 'probability': 0.25}, u'Argentina': {'count': 1, 'probability': 0.07142857142857142}, u'Georgia': {'count': 2, 'probability': 1.0}, u'Italy': {'count': 1, 'probability': 0.07142857142857142}, u'Uruguay': {'count': 1, 'probability': 0.07142857142857142}, u'Saint Lucia': {'count': 1, 'probability': 0.07142857142857142}, u'Mexico': {'count': 1, 'probability': 0.07142857142857142}, u'Costa Rica': {'count': 1, 'probability': 0.07142857142857142}, u'Venezuela, Bolivarian republic of': {'count': 1, 'probability': 0.07142857142857142}, u'United States': {'count': 10, 'probability': 1.0}, u'Colombia': {'count': 1, 'probability': 0.25}, u'Panama': {'count': 1, 'probability': 0.07142857142857142}, u'Philippines': {'count': 1, 'probability': 0.07142857142857142}, u'Spain': {'count': 1, 'probability': 0.07142857142857142}, u'Moldova, Republic of': {'count': 1, 'probability': 0.07142857142857142}}\n",
      "***\n",
      "1\n",
      "{u'United States': {'count': 2, 'probability': 1.0}}\n",
      "***\n",
      "2\n",
      "{u'Canada': {'count': 1, 'probability': 0.08333333333333333}, u'United Kingdom': {'count': 1, 'probability': 0.5833333333333334}, u'Romania': {'count': 1, 'probability': 0.08333333333333333}, u'Australia': {'count': 2, 'probability': 0.8333333333333334}, u'South Africa': {'count': 1, 'probability': 0.1}, u'Egypt': {'count': 1, 'probability': 0.08333333333333333}, u'United States': {'count': 7, 'probability': 1.0}, u'Ireland': {'count': 1, 'probability': 0.1}, u'Brazil': {'count': 1, 'probability': 0.1}}\n",
      "***\n",
      "3\n",
      "{u'United Kingdom': {'count': 1, 'probability': 0.25}, u'Kyrgyzstan': {'count': 7, 'probability': 1.0}, u'Georgia': {'count': 5, 'probability': 1.0}, u'United States': {'count': 10, 'probability': 1.0}, u'Colombia': {'count': 1, 'probability': 0.25}, u'Russian Federation': {'count': 6, 'probability': 1.0}}\n",
      "***\n",
      "4\n",
      "{u'United States': {'count': 7, 'probability': 0.999968}, u'Brazil': {'count': 1, 'probability': 0.6000000000000001}, u'Australia': {'count': 1, 'probability': 0.1}, u'Ireland': {'count': 1, 'probability': 0.1}, u'South Africa': {'count': 1, 'probability': 0.6000000000000001}}\n",
      "***\n",
      "5\n",
      "{u'United States': {'count': 3, 'probability': 0.9984375}, u'United Kingdom': {'count': 1, 'probability': 0.25}, u'Colombia': {'count': 1, 'probability': 0.25}}\n",
      "***\n",
      "6\n",
      "{u'United States': {'count': 3, 'probability': 0.9996744791666666}, u'Russian Federation': {'count': 1, 'probability': 0.16666666666666666}, u'China': {'count': 2, 'probability': 1.0}, u'United Kingdom': {'count': 1, 'probability': 0.25}, u'Colombia': {'count': 1, 'probability': 0.25}}\n",
      "***\n",
      "7\n",
      "{u'Brazil': {'count': 1, 'probability': 0.5}, u'Bonaire, sint eustatius and saba': {'count': 1, 'probability': 0.5}, u'Montenegro': {'count': 3, 'probability': 1.0}, u'Serbia': {'count': 2, 'probability': 1.0}, u'United States': {'count': 10, 'probability': 1.0}, u'South Sudan': {'count': 2, 'probability': 1.0}, u'Bonaire, Sint Eustatius And Saba': {'count': 1, 'probability': 1.0}}\n",
      "***\n",
      "8\n",
      "{u'United States': {'count': 16, 'probability': 1.0}, u'Nigeria': {'count': 3, 'probability': 1.0}, u'Australia': {'count': 1, 'probability': 0.5}, u'Portugal': {'count': 1, 'probability': 0.25}}\n",
      "***\n",
      "9\n",
      "{u'Brazil': {'count': 1, 'probability': 0.6666666666666667}, u'Qatar': {'count': 1, 'probability': 0.16666666666666666}, u'Australia': {'count': 1, 'probability': 0.16666666666666666}, u'Czech Republic': {'count': 1, 'probability': 0.16666666666666666}, u'Mexico': {'count': 1, 'probability': 0.2}, u'El Salvador': {'count': 1, 'probability': 0.16666666666666666}, u'Panama': {'count': 1, 'probability': 0.2}, u'Costa Rica': {'count': 1, 'probability': 0.2}, u'United States': {'count': 12, 'probability': 1.0}, u'Trinidad and Tobago': {'count': 1, 'probability': 0.16666666666666666}, u'New Zealand': {'count': 1, 'probability': 0.8}, u'Spain': {'count': 1, 'probability': 0.2}}\n",
      "***\n",
      "10\n",
      "{u'United States': {'count': 4, 'probability': 1.0}, u'Korea, Republic of': {'count': 2, 'probability': 1.0}}\n",
      "***\n",
      "11\n",
      "{u'United States': {'count': 12, 'probability': 1.0}, u'Philippines': {'count': 4, 'probability': 1.0}, u'San Marino': {'count': 3, 'probability': 1.0}}\n",
      "***\n",
      "12\n",
      "{u'United States': {'count': 15, 'probability': 1.0}, u'Nigeria': {'count': 3, 'probability': 1.0}, u'Australia': {'count': 1, 'probability': 0.5}}\n",
      "***\n",
      "13\n",
      "{u'United States': {'count': 24, 'probability': 1.0}, u'Canada': {'count': 8, 'probability': 1.0}, u'United Kingdom': {'count': 2, 'probability': 0.375}, u'Colombia': {'count': 1, 'probability': 0.25}}\n",
      "***\n",
      "14\n",
      "{u'United States': {'count': 9, 'probability': 1.0}, u'Russian Federation': {'count': 1, 'probability': 0.16666666666666666}, u'China': {'count': 2, 'probability': 1.0}, u'United Kingdom': {'count': 1, 'probability': 0.25}, u'Colombia': {'count': 1, 'probability': 0.25}}\n",
      "***\n",
      "15\n",
      "{u'United States': {'count': 15, 'probability': 1.0}, u'United Kingdom': {'count': 1, 'probability': 0.25}, u'Colombia': {'count': 1, 'probability': 0.25}}\n",
      "***\n",
      "16\n",
      "{u'United States': {'count': 2, 'probability': 0.95}, u'Australia': {'count': 1, 'probability': 0.25}}\n",
      "***\n",
      "17\n",
      "{u'Brazil': {'count': 1, 'probability': 1.0}, u'Korea, Republic of': {'count': 1, 'probability': 1.0}, u'United States': {'count': 4, 'probability': 1.0}, u'Argentina': {'count': 1, 'probability': 1.0}, u'South Africa': {'count': 1, 'probability': 1.0}, u'Mexico': {'count': 2, 'probability': 1.0}, u'India': {'count': 1, 'probability': 1.0}, u'United Kingdom': {'count': 1, 'probability': 0.25}, u'China': {'count': 5, 'probability': 1.0}, u'Colombia': {'count': 1, 'probability': 0.25}, u'Russian Federation': {'count': 2, 'probability': 1.0}, u'Thailand': {'count': 1, 'probability': 1.0}}\n",
      "***\n",
      "18\n",
      "{u'United States': {'count': 6, 'probability': 1.0}, u'Nigeria': {'count': 1, 'probability': 0.125}, u'Lebanon': {'count': 1, 'probability': 0.125}, u'Benin': {'count': 1, 'probability': 0.125}}\n",
      "***\n",
      "19\n",
      "{u'Canada': {'count': 6, 'probability': 1.0}, u'United Kingdom': {'count': 2, 'probability': 0.4375}, u'Qatar': {'count': 1, 'probability': 0.16666666666666666}, u'Australia': {'count': 2, 'probability': 0.375}, u'Italy': {'count': 1, 'probability': 0.5714285714285714}, u'Uruguay': {'count': 1, 'probability': 0.07142857142857142}, u'Iceland': {'count': 1, 'probability': 0.8}, u'Saint Lucia': {'count': 1, 'probability': 0.07142857142857142}, u'Mexico': {'count': 1, 'probability': 0.2}, u'Costa Rica': {'count': 1, 'probability': 0.2}, u'France': {'count': 1, 'probability': 0.75}, u'United States': {'count': 21, 'probability': 1.0}, u'Trinidad and Tobago': {'count': 1, 'probability': 0.16666666666666666}, u'Barbados': {'count': 1, 'probability': 0.25}, u'Brazil': {'count': 1, 'probability': 0.16666666666666666}, u'New Zealand': {'count': 1, 'probability': 0.8}, u'Panama': {'count': 1, 'probability': 0.2}, u'Spain': {'count': 2, 'probability': 0.657142857142857}, u'Moldova, Republic of': {'count': 1, 'probability': 0.07142857142857142}}\n",
      "***\n",
      "20\n",
      "{u'United States': {'count': 13, 'probability': 1.0}, u'Turkey': {'count': 3, 'probability': 1.0}, u'United Kingdom': {'count': 1, 'probability': 0.25}, u'Colombia': {'count': 1, 'probability': 0.25}}\n"
     ]
    }
   ],
   "source": [
    "sample = df.ix[0:20]\n",
    "for row in sample.iterrows():\n",
    "    print '***'\n",
    "    print row[0]\n",
    "    print parse_countries(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pbar = ProgressBar(maxval=df.shape[0]).start()\n",
    "countries = []\n",
    "for ix, row in enumerate(df.iterrows()):\n",
    "    countries.append(parse_countries(row[1]))\n",
    "    pbar.update(ix)\n",
    "pbar.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_text_url</th>\n",
       "      <th>title</th>\n",
       "      <th>toc_subject</th>\n",
       "      <th>topics</th>\n",
       "      <th>raw_text</th>\n",
       "      <th>entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>https://www.federalregister.gov/articles/text/...</td>\n",
       "      <td>Fresh Garlic From the People's Republic of Chi...</td>\n",
       "      <td>Antidumping Duty New Shipper Reviews; Results,...</td>\n",
       "      <td>[]</td>\n",
       "      <td>\\nSUMMARY: \\nThe Department of Commerce (Depar...</td>\n",
       "      <td>[(Department of Commerce ( Department, ORGANIZ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         raw_text_url  \\\n",
       "14  https://www.federalregister.gov/articles/text/...   \n",
       "\n",
       "                                                title  \\\n",
       "14  Fresh Garlic From the People's Republic of Chi...   \n",
       "\n",
       "                                          toc_subject topics  \\\n",
       "14  Antidumping Duty New Shipper Reviews; Results,...     []   \n",
       "\n",
       "                                             raw_text  \\\n",
       "14  \\nSUMMARY: \\nThe Department of Commerce (Depar...   \n",
       "\n",
       "                                             entities  \n",
       "14  [(Department of Commerce ( Department, ORGANIZ...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.raw_text_url == 'https://www.federalregister.gov/articles/text/raw_text/201/231/447.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({u'China': {'count': 2, 'probability': 1.0},\n",
       "  u'Colombia': {'count': 1, 'probability': 0.25},\n",
       "  u'Russian Federation': {'count': 1, 'probability': 0.16666666666666666},\n",
       "  u'United Kingdom': {'count': 1, 'probability': 0.25},\n",
       "  u'United States': {'count': 9, 'probability': 1.0}},\n",
       " {u'China': {'possible_countries': [u'United States',\n",
       "    u'Russian Federation',\n",
       "    u'China']},\n",
       "  u'DC': {'possible_countries': [u'United States', u'Colombia']},\n",
       "  u'Washington': {'possible_countries': [u'United States',\n",
       "    u'United Kingdom']}})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countries = get_countries(df.ix[14,'entities'])\n",
    "update_countries_with_regions(df.ix[14,'entities'], countries, df.ix[14,'raw_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "{u'China': {'count': 1, 'probability': 0.16666666666666666},\n",
    " u'Colombia': {'count': 1, 'probability': 0.25},\n",
    " u'Russian Federation': {'count': 1, 'probability': 0.16666666666666666},\n",
    " u'United Kingdom': {'count': 1, 'probability': 0.25},\n",
    " u'United States': {'count': 4, 'probability': 0.9999674479166667}}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
