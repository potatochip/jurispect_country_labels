{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "from multiprocessing import Pool\n",
    "from progressbar import ProgressBar\n",
    "from unidecode import unidecode\n",
    "import nltk\n",
    "from nltk.tag.stanford import StanfordNERTagger\n",
    "from nltk import pos_tag\n",
    "from nltk.chunk import conlltags2tree\n",
    "from nltk.tree import Tree\n",
    "import string\n",
    "import pycountry\n",
    "import jellyfish\n",
    "import csv\n",
    "from collections import Counter\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle('docs_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample = df.raw_text.ix[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "st = StanfordNERTagger('/Users/amangum/stanford-ner/classifiers/english.all.3class.distsim.crf.ser.gz', '/Users/amangum/stanford-ner/stanford-ner.jar')\n",
    "\n",
    "def stanfordNE2BIO(tagged_sent):\n",
    "    bio_tagged_sent = []\n",
    "    prev_tag = \"O\"\n",
    "    for token, tag in tagged_sent:\n",
    "        if tag == \"O\": #O\n",
    "            bio_tagged_sent.append((token, tag))\n",
    "            prev_tag = tag\n",
    "            continue\n",
    "        if tag != \"O\" and prev_tag == \"O\": # Begin NE\n",
    "            bio_tagged_sent.append((token, \"B-\"+tag))\n",
    "            prev_tag = tag\n",
    "        elif prev_tag != \"O\" and prev_tag == tag: # Inside NE\n",
    "            bio_tagged_sent.append((token, \"I-\"+tag))\n",
    "            prev_tag = tag\n",
    "        elif prev_tag != \"O\" and prev_tag != tag: # Adjacent NE\n",
    "            bio_tagged_sent.append((token, \"B-\"+tag))\n",
    "            prev_tag = tag\n",
    "\n",
    "    return bio_tagged_sent\n",
    "\n",
    "\n",
    "def stanfordNE2tree(ne_tagged_sent):\n",
    "    bio_tagged_sent = stanfordNE2BIO(ne_tagged_sent)\n",
    "    sent_tokens, sent_ne_tags = zip(*bio_tagged_sent)\n",
    "    sent_pos_tags = [pos for token, pos in pos_tag(sent_tokens)]\n",
    "\n",
    "    sent_conlltags = [(token, pos, ne) for token, pos, ne in zip(sent_tokens, sent_pos_tags, sent_ne_tags)]\n",
    "    ne_tree = conlltags2tree(sent_conlltags)\n",
    "    return ne_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_entities(text):\n",
    "    # stanford tagger takes 2.45 seconds vs 2.38 seconds for standard tagger\n",
    "    \n",
    "#     tokens = [w for w in nltk.word_tokenize(text) if w.isalpha()]\n",
    "#     tokens = nltk.word_tokenize(unidecode(text).translate(None, string.punctuation))\n",
    "#     tokens = text.split()\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    places = []\n",
    "\n",
    "    # stanford tagger\n",
    "    ne_tagged_sent = st.tag(tokens)\n",
    "    ne_tree = stanfordNE2tree(ne_tagged_sent)  \n",
    "    for ne in ne_tree:\n",
    "        if isinstance(ne, Tree): # If subtree is a noun chunk, i.e. NE != \"O\"\n",
    "            if ne.label() in ['LOCATION', 'PERSON', 'ORGANIZATION']:\n",
    "#             if ne.label() in ['LOCATION']:\n",
    "                ne_label = ne.label()\n",
    "                ne_string = u' '.join([token for token, pos in ne.leaves()])\n",
    "                places.append([ne_string, ne_label])\n",
    "                \n",
    "    c = Counter((entity, label) for entity, label in places)\n",
    "    return {(entity, label, count) for (entity, label), count in c.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# check whether any part of an entity string relates to a country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_non_ascii(s): return \"\".join(i for i in s if ord(i)<128)\n",
    " \n",
    "def fuzzy_match(s1, s2, max_dist=.8):\n",
    "    return jellyfish.jaro_distance(s1, s2) >= max_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def title_except(s, exceptions=['a', 'an', 'of', 'the', 'is']):\n",
    "    word_list = re.split(' ', s)       #re.split behaves as expected\n",
    "    final = [word_list[0].capitalize()]\n",
    "    for word in word_list[1:]:\n",
    "        final.append(word in exceptions and word or word.capitalize())\n",
    "    return \" \".join(final)\n",
    "\n",
    "\n",
    "def correct_country_mispelling(s):\n",
    "    with open(\"ISO3166ErrorDictionary.csv\", \"rb\") as info:\n",
    "        reader = csv.reader(info)\n",
    "        for row in reader:\n",
    "            try:\n",
    "                if unidecode(s.decode('utf8')).lower() == unidecode(row[0].decode('utf8')).lower():\n",
    "                    return row[2]\n",
    "            except:\n",
    "                #fails on unicode string\n",
    "                try:\n",
    "                    if unidecode(s).lower() == row[0].lower():\n",
    "                        return row[2]\n",
    "                except:\n",
    "                    pass\n",
    "            try:\n",
    "                if s.lower == unicode(row[0], 'utf8'):\n",
    "                    return row[2]\n",
    "            except:\n",
    "                try:\n",
    "                    # error on string\n",
    "                    if s.lower() == row[0].lower():\n",
    "                        return row[2]\n",
    "                except:\n",
    "                    pass\n",
    "            if s.lower() == remove_non_ascii(row[0]).lower():\n",
    "                return row[2]\n",
    "    return s\n",
    "\n",
    "\n",
    "def is_a_country(s):\n",
    "    s = correct_country_mispelling(s)\n",
    "    try:\n",
    "        pycountry.countries.get(name=s)\n",
    "        return True\n",
    "    except:\n",
    "        try:\n",
    "            pycountry.countries.get(name=title_except(s))\n",
    "            return True\n",
    "        except KeyError, e:\n",
    "            return False\n",
    "\n",
    "    \n",
    "def set_countries(places):\n",
    "    countries = [correct_country_mispelling(place) for place in places if is_a_country(place)]\n",
    "    country_mentions = Counter(countries).most_common()\n",
    "    return country_mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geoname_id</th>\n",
       "      <th>continent_code</th>\n",
       "      <th>continent_name</th>\n",
       "      <th>country_iso_code</th>\n",
       "      <th>country_name</th>\n",
       "      <th>subdivision_iso_code</th>\n",
       "      <th>subdivision_name</th>\n",
       "      <th>city_name</th>\n",
       "      <th>metro_code</th>\n",
       "      <th>time_zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1861060</td>\n",
       "      <td>AS</td>\n",
       "      <td>Asia</td>\n",
       "      <td>JP</td>\n",
       "      <td>Japan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Asia/Tokyo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1809858</td>\n",
       "      <td>AS</td>\n",
       "      <td>Asia</td>\n",
       "      <td>CN</td>\n",
       "      <td>China</td>\n",
       "      <td>44</td>\n",
       "      <td>Guangdong</td>\n",
       "      <td>Guangzhou</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Asia/Shanghai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1850147</td>\n",
       "      <td>AS</td>\n",
       "      <td>Asia</td>\n",
       "      <td>JP</td>\n",
       "      <td>Japan</td>\n",
       "      <td>13</td>\n",
       "      <td>Tōkyō</td>\n",
       "      <td>Tokyo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Asia/Tokyo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1814991</td>\n",
       "      <td>AS</td>\n",
       "      <td>Asia</td>\n",
       "      <td>CN</td>\n",
       "      <td>China</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2077456</td>\n",
       "      <td>OC</td>\n",
       "      <td>Oceania</td>\n",
       "      <td>AU</td>\n",
       "      <td>Australia</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   geoname_id continent_code continent_name country_iso_code country_name  \\\n",
       "0     1861060             AS           Asia               JP        Japan   \n",
       "1     1809858             AS           Asia               CN        China   \n",
       "2     1850147             AS           Asia               JP        Japan   \n",
       "3     1814991             AS           Asia               CN        China   \n",
       "4     2077456             OC        Oceania               AU    Australia   \n",
       "\n",
       "  subdivision_iso_code subdivision_name  city_name  metro_code      time_zone  \n",
       "0                  NaN              NaN        NaN         NaN     Asia/Tokyo  \n",
       "1                   44        Guangdong  Guangzhou         NaN  Asia/Shanghai  \n",
       "2                   13            Tōkyō      Tokyo         NaN     Asia/Tokyo  \n",
       "3                  NaN              NaN        NaN         NaN            NaN  \n",
       "4                  NaN              NaN        NaN         NaN            NaN  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cities = pd.DataFrame.from_csv('GeoLite2-City-Locations.csv', index_col=None)\n",
    "cities.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s1 = cities[['country_name', 'subdivision_name']].dropna().rename(columns={'subdivision_name':'subdivision'})\n",
    "s1['type'] = 'subdivision'\n",
    "s2 = cities[['country_name', 'subdivision_iso_code']].dropna().rename(columns={'subdivision_iso_code':'subdivision'})\n",
    "s2['type'] = 'subdivision_code'\n",
    "s3 = cities[['country_name', 'city_name']].dropna().rename(columns={'city_name':'subdivision'})\n",
    "s3['type'] = 'city'\n",
    "alles = pd.concat([s1,s2,s3], ignore_index=True).drop_duplicates()\n",
    "\n",
    "\n",
    "def update_labels_with_regions(places, labels):\n",
    "    subs = pd.DataFrame()\n",
    "    for place in places:\n",
    "        a = alles[alles.subdivision == place]\n",
    "        if not a.empty:\n",
    "            subs = pd.concat([subs, a], ignore_index=True)\n",
    "    \n",
    "    if not subs.empty:\n",
    "        no_dupes = subs.drop_duplicates(['country_name', 'subdivision'])\n",
    "        for value_count in no_dupes.subdivision.value_counts().iteritems():\n",
    "            probability = 1.0 / value_count[1]\n",
    "            if probability == 1.0:\n",
    "                # only one country exists for any probability\n",
    "                probability = 0.8 # correcting for imperfect entity parsing\n",
    "                possible_countries = subs[subs.subdivision == value_count[0]].country_name.tolist()\n",
    "                country = possible_countries[0]\n",
    "                if country in labels:\n",
    "                    priors = labels[country]\n",
    "                    new_count = priors['count'] + len(possible_countries)\n",
    "                    probability_non_occurrence = (1-priors['probability']) * (1-probability)\n",
    "                    new_probability = 1 - probability_non_occurrence\n",
    "                    labels.update({unicode(country, 'utf8'): {'count': new_count, 'probability': new_probability}})\n",
    "                else:\n",
    "                    labels.update({unicode(country, 'utf8'): {'count': len(possible_countries), 'probability': probability}})\n",
    "            else:\n",
    "                # multiple countries exist for a single subdivision\n",
    "                possible_countries = no_dupes[no_dupes.subdivision == value_count[0]].country_name.tolist()\n",
    "                for country in possible_countries:\n",
    "                    if country in labels:\n",
    "                        priors = labels[country]\n",
    "                        new_count = priors['count'] + 1\n",
    "                        probability_non_occurrence = (1-priors['probability']) * (1-probability)\n",
    "                        new_probability = 1 - probability_non_occurrence\n",
    "                        labels.update({unicode(country, 'utf8'): {'count': new_count, 'probability': new_probability}})\n",
    "                    else:\n",
    "                        labels.update({unicode(country, 'utf8'): {'count': 1, 'probability': probability}})\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amangum/anaconda/envs/datasci/lib/python2.7/site-packages/pandas/core/ops.py:562: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "  result = lib.scalar_compare(x, y, op)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{u'Nigeria': {'count': 2, 'probability': 1.0},\n",
       " u'United States': {'count': 12, 'probability': 1.0}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_labels(text):\n",
    "    places = find_entities(text)\n",
    "    country_mentions = set_countries(places)\n",
    "    labels = {i[0]: {'probability': 1.0, 'count': i[1]} for i in country_mentions}\n",
    "    labels = update_labels_with_regions(places, labels)\n",
    "    return labels\n",
    "\n",
    "get_labels(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pool = Pool()\n",
    "result = pool.map(get_labels, df.raw_text.tolist())\n",
    "pool.close()\n",
    "\n",
    "# countries = []\n",
    "# progress = ProgressBar()\n",
    "# for i in progress(df.raw_text.tolist()):\n",
    "#     places = get_labels(i)\n",
    "#     countries.append(places)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['countries_stanford_alpha'] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_pickle('parsed_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#discriminate ambiguous cities and subdivisons by surrounding states/countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "raw_text_url                https://www.federalregister.gov/articles/text/...\n",
       "title                       In the Matter of: Emenike Charles Nwankwoala C...\n",
       "toc_subject                                 Orders Denying Export Privileges:\n",
       "topics                                                                     []\n",
       "raw_text                    \\nOrder Denying Export Privileges \\nOn January...\n",
       "countries_stanford_alpha    {u'United States': {u'count': 12, u'probabilit...\n",
       "Name: 8, dtype: object"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.ix[8, 'countries_stanford_alpha"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
